{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/milindpatil/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages (from gym) (1.21.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/milindpatil/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages (from gym) (2.0.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/milindpatil/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages (from gym) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/milindpatil/miniforge3/envs/env_tensorflow/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym) (3.6.0)\n",
      "Installing collected packages: gym-notices, gym\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wvqHkiUxn5zc"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gym\n",
    "\n",
    "from gridworld import *\n",
    "# from analyze import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Aemga1sgoMWJ",
    "outputId": "3189cf07-7424-42fd-f106-11dda7d6617a"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'observation_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/vbrqfbx93j5g0qg2s07y9bsh0000gn/T/ipykernel_37350/3355356382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnumber_of_interestpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdimension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_robots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_interestpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnumber_of_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'observation_size'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #runtime = 100\n",
    "    filename_state = 'state'\n",
    "    filename_qtable = 'qtable'\n",
    "    filename_results = 'results'\n",
    "    number_of_robots = 1\n",
    "    number_of_interestpoints = 1\n",
    "    dimension = 4\n",
    "    env = GridWorld(number_of_robots, number_of_interestpoints, dimension)\n",
    "\n",
    "    number_of_episodes = 4000\n",
    "    env.train(number_of_episodes)\n",
    "    # analyze_convergence()\n",
    "    env.visualize_training(filename_state, number_of_episodes)\n",
    "    # env.evaluate(filename_qtable)\n",
    "    # env.visualize(filename_results)\n",
    "    env.close()\n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.suptitle('Plot Steps and Reward Buffer')\n",
    "    axs[0].plot(range(len(rew_buf)), rew_buf)\n",
    "    axs[1].plot(range(len(steps)), steps)\n",
    "    #plt.plot(range(len(rew_buf)),rew_buf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwJOrqkDqI2D"
   },
   "source": [
    "# Final Comments\n",
    "We can conclude from the above plots that the number of rewards keeps rising and essentially stays the same after a few episodes. Inconsistencies exist in the rewards between related episodes. 2. After each episode, the steps required to reach the interest point get progressively shorter, however there may be some variation. 3. The algorithm converges at a faster pace when the learning rate and discount factor are high and low, respectively. This results in fewer steps being needed and higher rewards. While the method takes longer to converge when the learning rate and discount rate are low and high, necessitating more steps and fewer rewards."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
